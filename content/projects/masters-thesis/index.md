+++ 
draft = false
date = 2025-11-27T12:46:55+01:00
title = "Masters Thesis: Towards Neural Encoding of Higher-Order Cognition with Large Language Models"
description = ""
slug = "masters-thesis"
authors = []
tags = []
categories = []
externalLink = ""
series = []
+++


# Abstract
Large Language Models (LLMs) have demonstrated remarkable language understanding capabilities, significantly advancing neural encoding models. While recent
progress has improved LLMs’ reasoning abilities, they still face limitations in social
contexts that require nuanced understanding of human cognition. This thesis investigates whether inference-time techniques like chain-of-thought reasoning—which
externalize an LLM’s reasoning process—can be leveraged for neural encoding of human reasoning and higher-order cognitive processing. Specifically, we focus on theory
of mind and social reasoning within conversational contexts, examining the detection
of subtext (unspoken beliefs, intentions, and desires) in communication.

Towards this we present two complementary projects: First, we develop a novel
Theory of Mind-inspired Tree of Thought approach for dialogue generation that explicitly models subtextual reasoning. Our method produces diverse, contextually
appropriate outputs, though automatic selection mechanisms currently struggle to
outperform single-step generation. Through perturbation analysis and evaluator consistency tests, we gain insights into model confidence and the distribution of plausible responses. Second, we apply LLM-derived embeddings to neural encoding tasks,
successfully modeling brain activity associated with both word- and sentence-level
language processing using banded-ridge regression.Our findings suggest the potential
for modifying sentence-level inputs to target the encoding of higher-order thinking in
high-frequency Electrocorticography (ECoG) data. Together, these projects highlight
both the promise and current limitations of LLMs in modeling social reasoning.

We conclude with proposals for future work, including improving training data,
context structuring, evaluation metrics, and thought representation. Our findings
point to new directions in aligning language models with human-like communicative
competence and mental state inference.



{{< pdf file="Sean_MEng_Thesis.pdf" >}}





[Download here](Sean_MEng_Thesis.pdf)