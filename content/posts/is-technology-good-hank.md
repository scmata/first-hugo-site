+++ 
draft = true
date = 2025-07-08T16:05:42+08:00
title = ""
description = ""
slug = ""
authors = []
tags = []
categories = []
externalLink = ""
series = []
+++


What I learnt from Hank Green's valedictorian speech at MIT

# Is technology good?

Addressing the MIT class of 2025 Hank Green said "When I asked you what you did at MIT you said built, but when I asked you what gave you hope, you didn't say buildings, you said people". His key thesis here is that you should not solely focus on building the most interesting or coolest technological advancement possible, but also focus on its impact.[^a] For the last month, I've been trying to figure out how exactly to interpret this.

As an engineer at the University of Oxford, I focused a lot my last four years on having the best technical ability I could. This was a demanding task with the Oxford courseload. As time passed, I began to think more about my future career and its impact. My main interests became biomedical engineering (liver transplant story) and AI, primarily because of their impact, capability and interestingness. After working in an F1 team, it felt easier to find value in my life by contributing to saving other people's through biomedical rather than in other ways such as making cars go faster round a track. This was the easy option though, because the space of careers is much bigger than the space of engineering careers, and there were many possiblities I had not considered at this point.

The key question for me was: what should I work on? To understand this, you need to understand the value of work and business in society[^b] and the different types of roles and how they contribute, this is not easy. In other careers such as law, politics and art this seems a lot easier because your roles are more directly tied to the benefit of people, but 


I had studied ethics like bribery and wistleblowing, but actually first moment I got an opportunity think about this with other people was my HCI class

To solve this I am understanding the shape of work


[^b]: which is why I recommend learning about business, reading some introductory economics

[^a]: and also that while building tools you should not get distracted by the scale of problems and of the world and still appreciate life on the lowest scale

orient yourself not just towards the construction and acquiustion of new and powerful tools, but to the needs of people

as a builder of not just tools, but of meaning.

"When I asked you what you did at MIT you said built, but when I asked you what gave you hope, you didn't say buildings, you said people"


Been thinking about this.

hank green valedictorian speech.

engineering is about efficiency. and innovation.

will building stuff generally help people?

I had the misconception of wanting to go into biomedical engineering because I doubted my ability to help people otherwise.

economics/productivity-mankiw




nukes

guns for romans.

the study

social media

people prefer immediate return,

start bad, then regulation helps

industrial revolution

war 

hacking

long term value
plastic surgery schools
metric

great innovators

archimedes
leonardo davinci

von neumann

targetting innovation or just innovation in general?

health/wealth/relationships framework

are people inherently good?

the framework from 100M offers

on some levels you have no choice how the technology you create is used.


Learning to use something as a tool is different to creating with it or advancing it. Takes different mindsets.

Also difference between humanities learning and engineering. Humanities you discuss ideas.

Engineering you figure out the best way to use tools to produce real world artefacts.
(also how to set up and manage organisations to produce things better)

Physics you get back to discussing ideas.

bohlin vs griswald separetly






work on hard problems ben kuhn

Understand the shape of work






https://80000hours.org/podcast/episodes/allan-dafoe-unstoppable-technology-human-agency-agi/?int_campaign=homepage#do-humans-control-technological-change-000917


Do humans control technological change? [00:09:17]
Rob Wiblin: Let’s open by talking about the work that you did in your previous incarnation, which was as an academic. So I think you did your thesis back in the early 2010s on technological determinism as the main focus. And I think the paper that came out of that opens with, “Who — if anyone — controls technological change?” What was the academic debate there that you were reacting to or trying to be a part of?

Allan Dafoe: So my academic trajectory had a number of chapters. The first was on technological determinism, which we can come to. Just for completeness, the second was on great power politics, and peace specifically — which actually led to a lot of work that I think continues to be relevant to the question of AI and AGI governance — and then I also did some statistics and causal inference work, which has some relevance to thinking about AI today.

Turning back to technological determinism, I would say I first came to this in undergrad, reflecting on what shapes history and how we can do good and how we can kind of steer the trajectory of developments in a positive direction.

And an insight I had was that history is not just the sum of all of our efforts. It’s not just that we all kind of push in different directions and then you take the sum and that’s what you get. Rather, there’s these general equilibrium effects that economists often talk about, where it may be that for every unit of effort you push in one direction, the system will kind of push back with an equal force, or sometimes a weaker force, sometimes a stronger force.

So when you’re in such a system, it’s very important to understand these structural dynamics. Why does the system sometimes resist efforts or sometimes amplify efforts? Why do you see these really astounding patterns in macrohistory?

For example, if you look at patterns of GDP growth, there’s these famous curves where, after the devastation of World War II, both Germany and Japan completely rebound within less than a decade and then kind of return to their prewar trajectory.

And we’ve seen Moore’s law, which is just an astounding trend. It’s not just that it continues, that transistor density is increasing exponentially; it’s very much a line — so you can predict where we’ll be quite precisely years in advance. We now have scaling laws which have given us sort of our generation’s Moore’s law, which again seems to allow us to predict years in advance how large the models will be, how capable — for example, on loss and so forth.

There’s a number of other macro phenomena that seem quite persistent. The growth of civilisation: I talked about or looked at these trends in what you can call the maximum — so maximum energy processing of a civilisation, also things like the height of buildings, the durability of materials. Really just most functional properties of technology over time have gotten more functional, so the speed of transportation and so forth.

Robert Wright, in summarising the literature, writes that, “archaeologists can’t help but notice that, as a rule, the deeper you dig, the simpler the society whose remains you find.” There’s more generally an observation, which is almost a truism, that certain kinds of technology are so complex or difficult that they come after other forms of technology. It’s hard to imagine nuclear power coming before coal power, for example. So there’s all these macro phenomena and trends in technology, and it’s important to explain them.

Now, this naive explanation would say if history is just the sum of what people try to achieve, then it’s human will that’s produced all these trends, including the reliable tick-tock of Moore’s law.

But not all the trends are positive. I know you’ve reflected on the agricultural revolution, which evidence suggests was not great for a lot of people. The median human, probably their health and welfare went down during this long stretch from the agricultural revolution to the Industrial Revolution. Of course it gave rise to inequality, warfare, and various other things. And there’s other trends that different societal groups resisted.

So in short, I don’t think the answer is history is just the sum of what people try to do. It depends of course on things like power, on timing, on the ecosystem of what’s functional and what’s possible and what’s not, on what technology enables. So I wanted to make sense of this.

Rob Wiblin: I guess the thing that we have to try to reconcile here is: on the one hand, we see these trends that seem like they’re not really responsive to any person’s particular decisions. It’s a little bit like the psychohistory in Foundation, where you just have these broad trends where everyone is just an ant in this broader process and it’s not obvious that anything that any particular individual did was able to shift things.

On the other hand, technology, at least so far, doesn’t have its own agency. It does seem like in fact it’s humans doing all of the actions that are producing these outcomes. And couldn’t they, in principle, if they really hated what was happening, try to shift it? We feel like we have agency right now over how society goes, or we feel that we at least have some agency.

So how do you reconcile this macro picture — where it seems like humans don’t have that much control over technology, at least historically — with the micro picture, where we feel like we do now. Am I understanding it right?




In Foragers, Farmers and Fossil Fuels Ian presents a provocative alternative: human culture gradually evolves towards whatever system of organisation allows a society to harvest the most energy, and we then conclude that system is the most virtuous one. Egalitarian values helped hunter-gatherers hunt and gather effectively. Once farming was developed, hierarchy proved to be the social structure that produced the most grain (and best repelled nomadic raiders). And in the modern era, democracy and individuality have proven to be more productive ways to collect and exploit fossil fuels.

